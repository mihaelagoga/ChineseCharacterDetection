{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02612a82-c6f4-44d7-af09-8cb89aedbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_jsonl_path = '/srv/data/lt2326-h25/lt2326-h25/a1/train.jsonl'\n",
    "\n",
    "annotations_data = {}\n",
    "\n",
    "with open(train_jsonl_path, 'r') as f:\n",
    "    for line in f:\n",
    "\n",
    "        image_annotations = json.loads(line)\n",
    "\n",
    "        image_id = image_annotations ['image_id']\n",
    "\n",
    "        annotations_data[image_id] = image_annotations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b00f3-188f-4b65-abd5-880ff6d2c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "info_json_path = '/srv/data/lt2326-h25/lt2326-h25/a1/info.json'\n",
    "\n",
    "print(f\"Loading master list from: {info_json_path}\")\n",
    "\n",
    "\n",
    "with open(info_json_path, 'r') as f:\n",
    "    info_data = json.load(f)\n",
    "\n",
    "\n",
    "official_training_list = info_data['train']\n",
    "\n",
    "\n",
    "official_training_filenames = {img['file_name'] for img in official_training_list}\n",
    "\n",
    "\n",
    "print(f\"Successfully found and processed {len(official_training_filenames)} official training file entries.\")\n",
    "\n",
    "\n",
    "print(f\"Example filename: {list(official_training_filenames)[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c4910-896e-4e27-867b-a6da26190e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "images_dir_path = '/srv/data/lt2326-h25/lt2326-h25/a1/images'\n",
    "\n",
    "\n",
    "available_image_filenames = {f for f in os.listdir(images_dir_path) if f.endswith('.jpg')}\n",
    "\n",
    "\n",
    "usable_filenames_set = official_training_filenames & available_image_filenames\n",
    "\n",
    "usable_filenames = list(usable_filenames_set)\n",
    "\n",
    "print(f\"\\nThere are a total of {len(usable_filenames)} Chinese character images to split.\")\n",
    "random.shuffle(usable_filenames)\n",
    "\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "\n",
    "total_count = len(usable_filenames)\n",
    "train_end_index = int(total_count * train_ratio)\n",
    "val_end_index = int(total_count * (train_ratio + val_ratio))\n",
    "\n",
    "train_files = usable_filenames[:train_end_index]\n",
    "val_files = usable_filenames[train_end_index:val_end_index]\n",
    "test_files = usable_filenames[val_end_index:]\n",
    "\n",
    "print(f\"Training set:   {len(train_files)} files\")\n",
    "print(f\"Validation set: {len(val_files)} files\")\n",
    "print(f\"Test set:       {len(test_files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8333b-4fb4-4e8a-bfc8-0e1ffdaa1ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_dataset(file_list, annotations_lookup, images_base_path):\n",
    "    \n",
    "    structured_data = []\n",
    "    for filename in file_list:\n",
    "        \n",
    "        image_id = os.path.splitext(filename)[0]\n",
    "        \n",
    "        \n",
    "        if image_id in annotations_lookup:\n",
    "            structured_data.append({\n",
    "                'image_path': os.path.join(images_base_path, filename),\n",
    "                'annotations': annotations_lookup[image_id]\n",
    "            })\n",
    "    return structured_data\n",
    "\n",
    "\n",
    "train_dataset = structure_dataset(train_files, annotations_data, images_dir_path)\n",
    "val_dataset = structure_dataset(val_files, annotations_data, images_dir_path)\n",
    "test_dataset = structure_dataset(test_files, annotations_data, images_dir_path)\n",
    "\n",
    "print(f\" The 'train_dataset' was created with {len(train_dataset)} pics.\")\n",
    "print(f\" The 'val_dataset'was created with {len(val_dataset)} pics.\")\n",
    "print(f\" The 'test_dataset' was created with {len(test_dataset)} pics.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7340aa-bd53-474d-9b78-62196f0f8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ChineseCharacterDataset(Dataset):\n",
    "    def __init__(self, dataset_list, annotations_data, window_size=256, stride=128, transform=None):\n",
    "        self.dataset_list = dataset_list\n",
    "        self.annotations_data = annotations_data\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "        self.cache = {}\n",
    "        \n",
    "        self.windows = []\n",
    "        print(\"Pre-calculating sliding windows\")\n",
    "        for item in tqdm(self.dataset_list):\n",
    "            h = item['annotations']['height']\n",
    "            w = item['annotations']['width']\n",
    "            \n",
    "            num_x = math.ceil((w - self.window_size) / self.stride) + 1 if w > self.window_size else 1\n",
    "            num_y = math.ceil((h - self.window_size) / self.stride) + 1 if h > self.window_size else 1\n",
    "            \n",
    "            for i in range(num_y):\n",
    "                for j in range(num_x):\n",
    "                    y = i * self.stride\n",
    "                    x = j * self.stride\n",
    "                    self.windows.append({'image_path': item['image_path'], 'x': x, 'y': y})\n",
    "        \n",
    "        print(f\"--- Created {len(self.windows)} total windows from {len(self.dataset_list)} images. ---\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window_info = self.windows[idx]\n",
    "        image_path = window_info['image_path']\n",
    "        \n",
    "        if image_path in self.cache:\n",
    "            image, mask = self.cache[image_path]\n",
    "        else:\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.float32)\n",
    "            image_id = image_path.split('/')[-1].replace('.jpg', '')\n",
    "            if image_id in self.annotations_data:\n",
    "                for char_list in self.annotations_data[image_id]['annotations']:\n",
    "                    if isinstance(char_list, list) and len(char_list) > 0:\n",
    "                        for char_annotation in char_list:\n",
    "                            if char_annotation.get('is_chinese', False):\n",
    "                                polygon = np.array(char_annotation['polygon'], dtype=np.int32).reshape(-1, 2)\n",
    "                                cv2.fillPoly(mask, [polygon], 1)\n",
    "            \n",
    "            self.cache[image_path] = (image, mask)\n",
    "\n",
    "        x, y = window_info['x'], window_info['y']\n",
    "        h, w, _ = image.shape\n",
    "        \n",
    "        x_end = min(x + self.window_size, w)\n",
    "        y_end = min(y + self.window_size, h)\n",
    "        \n",
    "        image_patch = image[y:y_end, x:x_end]\n",
    "        mask_patch = mask[y:y_end, x:x_end]\n",
    "\n",
    "        pad_x = self.window_size - image_patch.shape[1]\n",
    "        pad_y = self.window_size - image_patch.shape[0]\n",
    "        if pad_x > 0 or pad_y > 0:\n",
    "            image_patch = np.pad(image_patch, ((0, pad_y), (0, pad_x), (0, 0)), mode='constant')\n",
    "            mask_patch = np.pad(mask_patch, ((0, pad_y), (0, pad_x)), mode='constant')\n",
    "\n",
    "        image_tensor = T.ToTensor()(image_patch)\n",
    "        mask_tensor = torch.from_numpy(mask_patch.copy()).unsqueeze(0).float()\n",
    "            \n",
    "        if self.transform:\n",
    "            image_tensor = self.transform(image_tensor)\n",
    "            \n",
    "        return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a9217-2380-4da8-af6f-dbcee696438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(dataset_list, image_dir, resize_shape=(256, 256)):\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation of the training dataset for normalization.\n",
    "    This prevents data leakage from the validation/test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    count = len(dataset_list)\n",
    "    mean = torch.empty(3, dtype=torch.float64)\n",
    "    std = torch.empty(3, dtype=torch.float64)\n",
    "    \n",
    "    temp_dataset = ChineseCharacterDataset(dataset_list)\n",
    "    loader = DataLoader(temp_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    sum_of_pixels = torch.zeros(3)\n",
    "    sum_of_squares = torch.zeros(3)\n",
    "    num_pixels = 0\n",
    "\n",
    "    for images, _ in tqdm(loader, desc=\"Calculating Stats\"):\n",
    "        # images shape is (batch, channels, height, width)\n",
    "        batch_size, channels, height, width = images.shape\n",
    "        num_pixels += batch_size * height * width\n",
    "        \n",
    "        sum_of_pixels += torch.sum(images, dim=[0, 2, 3])\n",
    "        sum_of_squares += torch.sum(images ** 2, dim=[0, 2, 3])\n",
    "\n",
    "    mean = sum_of_pixels / num_pixels\n",
    "    std = torch.sqrt((sum_of_squares / num_pixels) - mean ** 2)\n",
    "    \n",
    "    print(f\"Calculation Complete.\\nMean: {mean}\\nStd: {std}\")\n",
    "    return mean.tolist(), std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b1bb3-3e0a-45d5-a1c7-fdfd6d14b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleSegmentationModel(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "      \n",
    "        super().__init__()\n",
    "        \n",
    "       \n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.upsampler = nn.Sequential(\n",
    "            \n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "           \n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=2, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.final_conv = nn.Conv2d(16, 1, kernel_size=1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "       \n",
    "        encoded_features = self.encoder(x)\n",
    "        \n",
    "       \n",
    "        upsampled_features = self.upsampler(encoded_features)\n",
    "        \n",
    "        \n",
    "        logits = self.final_conv(upsampled_features)\n",
    "        \n",
    "        \n",
    "        output_mask = self.sigmoid(logits)\n",
    "        \n",
    "        return output_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5950761-7bb7-4a39-b89f-00cfef28d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"Preparing for Simple Model Training\")\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPOCHS = 15        \n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (512, 512) \n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_data_simple = ChineseCharacterDataset(train_dataset,annotations_data)\n",
    "val_data_simple = ChineseCharacterDataset(val_dataset,annotations_data)\n",
    "\n",
    "train_loader_simple = DataLoader(train_data_simple, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_simple = DataLoader(val_data_simple, batch_size=BATCH_SIZE)\n",
    "\n",
    "def calculate_pos_weight(dataloader):\n",
    "    print(\"Calculating pos_weight for BCE loss\")\n",
    "    num_pos = 0\n",
    "    num_neg = 0\n",
    "    for _, masks in tqdm(dataloader, desc=\"Calculating pos_weight\"):\n",
    "        num_pos += torch.sum(masks == 1)\n",
    "        num_neg += torch.sum(masks == 0)\n",
    "    return num_neg / num_pos\n",
    "\n",
    "pos_weight = calculate_pos_weight(train_loader_simple)\n",
    "print(f\"Positive weight calculated: {pos_weight:.2f}\")\n",
    "\n",
    "print(\"Initializing Model, Loss Function, and Optimizer\")\n",
    "model_1 = SimpleSegmentationModel().to(device)\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight).to(device))\n",
    "optimizer = optim.Adam(model_1.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef0121-81cd-4e15-85d3-fcdab3005492",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loss_history = []\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_1.train() \n",
    "    running_train_loss = 0.0\n",
    "    train_progress_bar = tqdm(train_loader_simple, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n",
    "    \n",
    "    for images, masks in train_progress_bar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        outputs = model_1(images)\n",
    "        loss = loss_function(outputs, masks)\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()       \n",
    "        optimizer.step()     \n",
    "        running_train_loss += loss.item()\n",
    "        train_progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader_simple)\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] - Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining finished\")\n",
    "MODEL_SAVE_PATH = \"simple_segmentation_model_improved.pth\"\n",
    "torch.save(model_1.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"\\nModel 1 has been trained and saved to {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ffa7c8-091f-408a-8385-d8ba762b2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    " # inspired from https://www.digitalocean.com/community/tutorials/writing-resnet-from-scratch-in-pytorch \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c727c9-b9d7-49d1-b86c-c8d639ccc629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class UNetResNetDeepSupervision(nn.Module):\n",
    "    def __init__(self, n_classes=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.base_model = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        self.encoder_layers = list(self.base_model.children())\n",
    "        self.layer0 = nn.Sequential(*self.encoder_layers[:3])\n",
    "        self.layer1 = nn.Sequential(*self.encoder_layers[3:5])\n",
    "        self.layer2 = self.encoder_layers[5]\n",
    "        self.layer3 = self.encoder_layers[6]\n",
    "        self.layer4 = self.encoder_layers[7]\n",
    "\n",
    "        # Decoder blocks\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec_conv3 = self._make_decoder_block(256 + 256, 256)\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec_conv2 = self._make_decoder_block(128 + 128, 128)\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec_conv1 = self._make_decoder_block(64 + 64, 64)\n",
    "        \n",
    "        self.upconv0 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec_conv0 = self._make_decoder_block(32 + 64, 32)\n",
    "        \n",
    "        self.final_upconv = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)\n",
    "        \n",
    "        self.out_final = nn.Conv2d(16, n_classes, kernel_size=1)\n",
    "        self.out_1 = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "        self.out_2 = nn.Conv2d(128, n_classes, kernel_size=1)\n",
    "        self.out_3 = nn.Conv2d(256, n_classes, kernel_size=1)\n",
    "\n",
    "    def _make_decoder_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        e0 = self.layer0(x); e1 = self.layer1(e0)\n",
    "        e2 = self.layer2(e1); e3 = self.layer3(e2); e4 = self.layer4(e3)\n",
    "\n",
    "        d3 = self.upconv3(e4); d3 = torch.cat([d3, e3], dim=1); d3 = self.dec_conv3(d3)\n",
    "        d2 = self.upconv2(d3); d2 = torch.cat([d2, e2], dim=1); d2 = self.dec_conv2(d2)\n",
    "        d1 = self.upconv1(d2); d1 = torch.cat([d1, e1], dim=1); d1 = self.dec_conv1(d1)\n",
    "        d0 = self.upconv0(d1); d0 = torch.cat([d0, e0], dim=1); d0 = self.dec_conv0(d0)\n",
    "        \n",
    "        final_features = self.final_upconv(d0)\n",
    "        \n",
    "        out_final = self.out_final(final_features)\n",
    "        \n",
    "        if self.training:\n",
    "            out1 = self.out_1(d1)\n",
    "            out2 = self.out_2(d2)\n",
    "            out3 = self.out_3(d3)\n",
    "            return out_final, out1, out2, out3\n",
    "        else:\n",
    "            return out_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df16452-1197-4470-a741-04062e417022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss for image segmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1.):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        probs_flat = probs.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "        intersection = (probs_flat * targets_flat).sum()\n",
    "        dice_coefficient = (2. * intersection + self.smooth) / (probs_flat.sum() + targets_flat.sum() + self.smooth)\n",
    "        return 1 - dice_coefficient\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss, to address extreme class imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        probs = torch.sigmoid(logits)\n",
    "        pt = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_term = (1.0 - pt).pow(self.gamma)\n",
    "        \n",
    "        alpha_term = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        \n",
    "        loss = alpha_term * focal_term * bce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "class FocalDiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    A combined loss that uses Focal Loss for pixel-wise classification\n",
    "    and Dice Loss for improving segmentation boundary definitions\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, smooth=1.):\n",
    "        super(FocalDiceLoss, self).__init__()\n",
    "        self.focal = FocalLoss(alpha=alpha, gamma=gamma)\n",
    "        self.dice = DiceLoss(smooth=smooth)\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        return self.focal(logits, targets) + self.dice(logits, targets)\n",
    "\n",
    "class DeepSupervisionLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates a weighted loss from the multiple outputs of a deeply supervised model.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_loss_fn, weights=None):\n",
    "        super(DeepSupervisionLoss, self).__init__()\n",
    "        self.base_loss_fn = base_loss_fn\n",
    "        self.weights = weights if weights is not None else [0.5 / (2**i) for i in range(10)]\n",
    "        self.weights.insert(0, 1.0)\n",
    "\n",
    "    def forward(self, outputs, masks):\n",
    "        \n",
    "        total_loss = self.weights[0] * self.base_loss_fn(outputs[0], masks)\n",
    "        \n",
    "        for i in range(1, len(outputs)):\n",
    "            downsampled_mask = F.interpolate(masks, size=outputs[i].shape[2:], mode='nearest')\n",
    "            \n",
    "            auxiliary_loss = self.weights[i] * self.base_loss_fn(outputs[i], downsampled_mask)\n",
    "            total_loss += auxiliary_loss\n",
    "            \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855908c4-9727-483e-a591-6221965ddf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model_epoch_ds(model, dataloader, optimizer, loss_function, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training Epoch\")\n",
    "    for images, masks in progress_bar:\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def validate_model_epoch_ds(model, dataloader, loss_function, device):\n",
    "    \"\"\"\n",
    "    Performs one full epoch of validation for the deep supervision model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = loss_function(outputs, masks)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7cb98e-1249-4f66-9683-8a6af6d1d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "import gc\n",
    "\n",
    "# Hyperparameters\n",
    "FREEZE_EPOCHS = 1\n",
    "UNFREEZE_EPOCHS = 2\n",
    "TOTAL_EPOCHS = FREEZE_EPOCHS + UNFREEZE_EPOCHS\n",
    "LEARNING_RATE_DECODER = 0.001\n",
    "LEARNING_RATE_FINETUNE = 0.00005\n",
    "IMAGE_SIZE = (768, 768)\n",
    "BATCH_SIZE = 16\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"--- Training Deep Supervision model on: {device} ---\")\n",
    "\n",
    "# Official ImageNet statistics\n",
    "imagenet_mean = [0.485, 0.456, 0.0406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(), T.RandomRotation(10),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    T.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])\n",
    "val_test_transform = T.Compose([T.Normalize(mean=imagenet_mean, std=imagenet_std)])\n",
    "train_data = ChineseCharacterDataset(train_dataset, annotations_data,transform=train_transform,)\n",
    "val_data = ChineseCharacterDataset(val_dataset, annotations_data, transform=val_test_transform)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "final_model = UNetResNetDeepSupervision().to(device) \n",
    "base_loss = FocalDiceLoss().to(device)\n",
    "loss_fn = DeepSupervisionLoss(base_loss_fn=base_loss).to(device)\n",
    "\n",
    "try:\n",
    "    print(f\"\\n--- STAGE 1: Freezing encoder and training decoder for {FREEZE_EPOCHS} epochs ---\")\n",
    "    for param in final_model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    optimizer = optim.AdamW(final_model.parameters(), lr=LEARNING_RATE_DECODER)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3, verbose=True)\n",
    "\n",
    "    best_val_loss = np.Inf\n",
    "    epochs_no_improve = 0\n",
    "    MODEL_SAVE_PATH = \"deep_supervision_model_best.pth\"\n",
    "\n",
    "    for epoch in range(FREEZE_EPOCHS):\n",
    "        avg_train_loss = train_model_epoch_ds(final_model, train_loader, optimizer, loss_fn, device)\n",
    "        avg_val_loss = validate_model_epoch_ds(final_model, val_loader, base_loss, device) # Note: val uses base_loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{TOTAL_EPOCHS}] -> Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(final_model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"  -> Checkpoint saved! New best validation loss: {best_val_loss:.4f}\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  -> Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n",
    "        \n",
    "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\n--- Early stopping triggered after {EARLY_STOPPING_PATIENCE} epochs with no improvement. ---\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\n--- STAGE 2: Unfreezing encoder and fine-tuning all layers for {UNFREEZE_EPOCHS} epochs ---\")\n",
    "    for param in final_model.base_model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    final_model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "    optimizer = optim.AdamW(final_model.parameters(), lr=LEARNING_RATE_FINETUNE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=3, verbose=True)\n",
    "    epochs_no_improve = 0 \n",
    "\n",
    "    for epoch in range(UNFREEZE_EPOCHS):\n",
    "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            break\n",
    "            \n",
    "        avg_train_loss = train_model_epoch_ds(final_model, train_loader, optimizer, loss_fn, device)\n",
    "        avg_val_loss = validate_model_epoch_ds(final_model, val_loader, base_loss, device)\n",
    "        scheduler.step(avg_val_loss)\n",
    "        print(f\"Epoch [{epoch+1+FREEZE_EPOCHS}/{TOTAL_EPOCHS}] -> Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(final_model.state_dict(), MODEL_SAVE_PATH)\n",
    "            print(f\"  -> Checkpoint saved! New best validation loss: {best_val_loss:.4f}\")\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  -> Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n",
    "        \n",
    "        if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\n--- Early stopping triggered after {EARLY_STOPPING_PATIENCE} epochs with no improvement. ---\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n--- Training Finished ---\")\n",
    "    print(f\"The best model was saved to {MODEL_SAVE_PATH} with a final validation loss of {best_val_loss:.4f}\")\n",
    "\n",
    "finally:\n",
    "    # Cleanup block\n",
    "    print(\"\\n--- Finalizing session and cleaning up CUDA memory... ---\")\n",
    "    try:\n",
    "        del final_model, train_loader, val_loader, train_data, val_data\n",
    "    except NameError:\n",
    "        print(\"Some objects were not defined, skipping deletion.\")\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"--- Cleanup complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4276f2-a237-470c-a47f-d61e4424d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_threshold(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Iterates through a range of thresholds to find the one that maximizes the F1-score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds_flat = []\n",
    "    all_masks_flat = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Gathering predictions for threshold tuning\"):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            \n",
    "            all_preds_flat.append(preds.view(-1))\n",
    "            all_masks_flat.append(masks.view(-1))\n",
    "\n",
    "    all_preds_flat = torch.cat(all_preds_flat)\n",
    "    all_masks_flat = torch.cat(all_masks_flat)\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0\n",
    "    \n",
    "    for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "        preds_binary = (all_preds_flat > threshold).float()\n",
    "        \n",
    "        tp = (preds_binary * all_masks_flat).sum().item()\n",
    "        fp = (preds_binary * (1 - all_masks_flat)).sum().item()\n",
    "        fn = ((1 - preds_binary) * all_masks_flat).sum().item()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "            \n",
    "    print(f\"Optimal threshold found: {best_threshold:.2f} (with F1-score: {best_f1:.4f})\")\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1771b4e-370c-4b42-adeb-2738a0409e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "\n",
    "def visualize_sw_comparison(simple_model, sw_model, dataset_list, device, num_samples=3, simple_model_size=(512, 512), window_size=256, stride=128, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Visualize a comparison between the simple model (full image) and the \n",
    "    advanced model (sliding window inference).\n",
    "    \"\"\"\n",
    "    simple_model.eval()\n",
    "    sw_model.eval()\n",
    "\n",
    "    fig, ax = plt.subplots(num_samples, 4, figsize=(20, 5 * num_samples))\n",
    "    fig.suptitle('Simple Model vs. Sliding Window Model Comparison', fontsize=20, y=1.02)\n",
    "    \n",
    "    sample_items = random.sample(dataset_list, num_samples)\n",
    "    \n",
    "    for i, item in enumerate(sample_items):\n",
    "        image_path = item['image_path']\n",
    "        image_id = image_path.split('/')[-1].replace('.jpg', '')\n",
    "        \n",
    "        original_image = cv2.imread(image_path)\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = original_image.shape\n",
    "        \n",
    "        gt_mask_full = np.zeros((h, w), dtype=np.uint8)\n",
    "        if image_id in annotations_data:\n",
    "            for char_list in annotations_data[image_id]['annotations']:\n",
    "                if isinstance(char_list, list) and len(char_list) > 0:\n",
    "                    for char_annotation in char_list:\n",
    "                        if char_annotation.get('is_chinese', False):\n",
    "                            polygon = np.array(char_annotation['polygon'], dtype=np.int32).reshape(-1, 2)\n",
    "                            cv2.fillPoly(gt_mask_full, [polygon], 1)\n",
    "\n",
    "        resized_for_simple = cv2.resize(original_image, simple_model_size)\n",
    "        simple_tensor = T.ToTensor()(resized_for_simple).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            pred_simple_raw = simple_model(simple_tensor)\n",
    "            pred_simple_prob = torch.sigmoid(pred_simple_raw).squeeze().cpu().numpy()\n",
    "        pred_simple_full = cv2.resize(pred_simple_prob, (w, h))\n",
    "\n",
    "        final_sw_mask = np.zeros((h, w), dtype=np.float32)\n",
    "        counts_mask = np.zeros((h, w), dtype=np.float32)\n",
    "\n",
    "        for y in range(0, h, stride):\n",
    "            for x in range(0, w, stride):\n",
    "                x_end, y_end = min(x + window_size, w), min(y + window_size, h)\n",
    "                image_patch = original_image[y:y_end, x:x_end]\n",
    "                \n",
    "                pad_x = window_size - image_patch.shape[1]\n",
    "                pad_y = window_size - image_patch.shape[0]\n",
    "                if pad_x > 0 or pad_y > 0:\n",
    "                    image_patch = np.pad(image_patch, ((0, pad_y), (0, pad_x), (0, 0)), mode='constant')\n",
    "\n",
    "                patch_tensor = T.ToTensor()(image_patch).unsqueeze(0).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    pred_patch_raw = sw_model(patch_tensor)\n",
    "                    pred_patch_prob = torch.sigmoid(pred_patch_raw).squeeze().cpu().numpy()\n",
    "                \n",
    "                pred_patch_prob = pred_patch_prob[:y_end-y, :x_end-x]\n",
    "                final_sw_mask[y:y_end, x:x_end] += pred_patch_prob\n",
    "                counts_mask[y:y_end, x:x_end] += 1\n",
    "        \n",
    "        final_sw_mask /= np.maximum(counts_mask, 1)\n",
    "\n",
    "        ax[i, 0].imshow(original_image); ax[i, 0].set_title(f\"Sample {i+1}: Original\"); ax[i, 0].axis('off')\n",
    "        ax[i, 1].imshow(gt_mask_full, cmap='gray'); ax[i, 1].set_title(\"Ground Truth\"); ax[i, 1].axis('off')\n",
    "        ax[i, 2].imshow(pred_simple_full > threshold, cmap='gray'); ax[i, 2].set_title(\"Simple Model Pred.\"); ax[i, 2].axis('off')\n",
    "        ax[i, 3].imshow(final_sw_mask > threshold, cmap='gray'); ax[i, 3].set_title(\"Sliding Window Pred.\"); ax[i, 3].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_metrics_simple(simple_model, dataset_list, device, resize_shape=(512, 512), threshold=0.5):\n",
    "    simple_model.eval()\n",
    "    total_tp, total_fp, total_fn, total_tn = 0, 0, 0, 0\n",
    "\n",
    "    for item in tqdm(dataset_list, desc=\"Metrics for Simple Model\"):\n",
    "        original_image = cv2.imread(item['image_path'])\n",
    "        h, w, _ = original_image.shape\n",
    "        gt_mask_full = np.zeros((h, w), dtype=np.uint8)\n",
    "        image_id = item['image_path'].split('/')[-1].replace('.jpg', '')\n",
    "        if image_id in annotations_data:\n",
    "            for char_list in annotations_data[image_id]['annotations']:\n",
    "                if isinstance(char_list, list) and len(char_list) > 0:\n",
    "                    for char_annotation in char_list:\n",
    "                        if char_annotation.get('is_chinese', False):\n",
    "                            polygon = np.array(char_annotation['polygon'], dtype=np.int32).reshape(-1, 2)\n",
    "                            cv2.fillPoly(gt_mask_full, [polygon], 1)\n",
    "        \n",
    "        resized_image = cv2.resize(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB), resize_shape)\n",
    "        tensor_image = T.ToTensor()(resized_image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = simple_model(tensor_image)\n",
    "            probs = torch.sigmoid(output).squeeze().cpu().numpy()\n",
    "        \n",
    "        pred_full_size = cv2.resize(probs, (w, h))\n",
    "        pred_binary = (pred_full_size > threshold).astype(np.uint8)\n",
    "\n",
    "        total_tp += (pred_binary * gt_mask_full).sum()\n",
    "        total_fp += (pred_binary * (1 - gt_mask_full)).sum()\n",
    "        total_fn += ((1 - pred_binary) * gt_mask_full).sum()\n",
    "        total_tn += ((1 - pred_binary) * (1 - gt_mask_full)).sum()\n",
    "        \n",
    "    accuracy = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn) if (total_tp + total_tn + total_fp + total_fn) > 0 else 0\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "def calculate_metrics_sw(sw_model, dataset_list, device, window_size=256, stride=128, threshold=0.5):\n",
    "    sw_model.eval()\n",
    "    total_tp, total_fp, total_fn, total_tn = 0, 0, 0, 0\n",
    "\n",
    "    for item in tqdm(dataset_list, desc=\"Metrics for Sliding Window Model\"):\n",
    "        original_image = cv2.imread(item['image_path'])\n",
    "        h, w, _ = original_image.shape\n",
    "        gt_mask_full = np.zeros((h, w), dtype=np.uint8)\n",
    "        image_id = item['image_path'].split('/')[-1].replace('.jpg', '')\n",
    "        if image_id in annotations_data:\n",
    "            for char_list in annotations_data[image_id]['annotations']:\n",
    "                if isinstance(char_list, list) and len(char_list) > 0:\n",
    "                    for char_annotation in char_list:\n",
    "                        if char_annotation.get('is_chinese', False):\n",
    "                            polygon = np.array(char_annotation['polygon'], dtype=np.int32).reshape(-1, 2)\n",
    "                            cv2.fillPoly(gt_mask_full, [polygon], 1)\n",
    "        \n",
    "        final_sw_mask = np.zeros((h, w), dtype=np.float32)\n",
    "        counts_mask = np.zeros((h, w), dtype=np.float32)\n",
    "        rgb_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        for y in range(0, h, stride):\n",
    "            for x in range(0, w, stride):\n",
    "                x_end, y_end = min(x + window_size, w), min(y + window_size, h)\n",
    "                image_patch = rgb_image[y:y_end, x:x_end]\n",
    "                \n",
    "                pad_x = window_size - image_patch.shape[1]\n",
    "                pad_y = window_size - image_patch.shape[0]\n",
    "                if pad_x > 0 or pad_y > 0:\n",
    "                    image_patch = np.pad(image_patch, ((0, pad_y), (0, pad_x), (0, 0)), mode='constant')\n",
    "\n",
    "                patch_tensor = T.ToTensor()(image_patch).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    pred_patch_raw = sw_model(patch_tensor)\n",
    "                    pred_patch_prob = torch.sigmoid(pred_patch_raw).squeeze().cpu().numpy()\n",
    "                \n",
    "                pred_patch_prob = pred_patch_prob[:y_end-y, :x_end-x]\n",
    "                final_sw_mask[y:y_end, x:x_end] += pred_patch_prob\n",
    "                counts_mask[y:y_end, x:x_end] += 1\n",
    "        \n",
    "        final_sw_mask /= np.maximum(counts_mask, 1)\n",
    "        pred_binary = (final_sw_mask > threshold).astype(np.uint8)\n",
    "\n",
    "        total_tp += (pred_binary * gt_mask_full).sum()\n",
    "        total_fp += (pred_binary * (1 - gt_mask_full)).sum()\n",
    "        total_fn += ((1 - pred_binary) * gt_mask_full).sum()\n",
    "        total_tn += ((1 - pred_binary) * (1 - gt_mask_full)).sum()\n",
    "\n",
    "    accuracy = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn) if (total_tp + total_tn + total_fp + total_fn) > 0 else 0\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"--- Running Final Evaluation on device: {device} ---\")\n",
    "\n",
    "SIMPLE_MODEL_SIZE = (512, 512)\n",
    "SW_WINDOW_SIZE = 256\n",
    "SW_STRIDE = 128\n",
    "\n",
    "loaded_simple_model = None\n",
    "loaded_final_model = None\n",
    "\n",
    "try:\n",
    "    print(\"Loading SimpleSegmentationModel...\")\n",
    "    loaded_simple_model = SimpleSegmentationModel().to(device)\n",
    "    simple_model_path = \"simple_segmentation_model_improved.pth\"\n",
    "    loaded_simple_model.load_state_dict(torch.load(simple_model_path, map_location=device))\n",
    "    print(f\"-> Successfully loaded model from: {simple_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load simple model: {e}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nLoading Sliding Window U-Net Model...\")\n",
    "    loaded_final_model = UNetResNetDeepSupervision().to(device)\n",
    "    final_model_path = \"deep_supervision_model_best.pth\"\n",
    "    loaded_final_model.load_state_dict(torch.load(final_model_path, map_location=device))\n",
    "    print(f\"-> Successfully loaded model from: {final_model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load final model: {e}\")\n",
    "\n",
    "if loaded_simple_model and loaded_final_model:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Visual Comparison of Model Predictions\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    visualize_sw_comparison(\n",
    "        loaded_simple_model, \n",
    "        loaded_final_model, \n",
    "        test_dataset, \n",
    "        device=device,\n",
    "        simple_model_size=SIMPLE_MODEL_SIZE,\n",
    "        window_size=SW_WINDOW_SIZE,\n",
    "        stride=SW_STRIDE\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Quantitative Metrics Comparison on the Full Test Set\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    acc1, pre1, rec1, f1_1 = calculate_metrics_simple(\n",
    "        loaded_simple_model, \n",
    "        test_dataset, \n",
    "        device,\n",
    "        resize_shape=SIMPLE_MODEL_SIZE\n",
    "    )\n",
    "    print(f\"\\nResults for SimpleSegmentationModel:\")\n",
    "    print(f\"  Accuracy:  {acc1:.4f}\")\n",
    "    print(f\"  Precision: {pre1:.4f}\")\n",
    "    print(f\"  Recall:    {rec1:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1_1:.4f}\")\n",
    "\n",
    "    acc2, pre2, rec2, f1_2 = calculate_metrics_sw(\n",
    "        loaded_final_model, \n",
    "        test_dataset, \n",
    "        device, \n",
    "        window_size=SW_WINDOW_SIZE,\n",
    "        stride=SW_STRIDE\n",
    "    )\n",
    "    print(f\"\\nResults for ResNet model with Sliding Window technique\")\n",
    "    print(f\"  Accuracy:  {acc2:.4f}\")\n",
    "    print(f\"  Precision: {pre2:.4f}\")\n",
    "    print(f\"  Recall:    {rec2:.4f}\")\n",
    "    print(f\"  F1-Score:  {f1_2:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nEvaluation skipped because one or both models could not be loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0bbdb5-7203-41a9-9949-05ed67dd9aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU] Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
